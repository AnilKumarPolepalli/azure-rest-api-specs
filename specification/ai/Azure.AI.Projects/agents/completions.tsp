import "./messages.tsp";

model CompletionsOptions {
  @doc("Strategy for truncating messages when input exceeds model limits.")
  truncationStrategy?: TruncationStrategy;
}

model TruncationStrategy {
  @doc("The type of truncation strategy to apply.")
  type: "auto" | "lastMessages";

  @doc("The number of most recent messages to retain when using 'lastMessages' strategy.")
  lastMessages?: int32;
}

model CompleteParams {
  @doc("The agent responsible for generating the completion.")
  agent: AgentOptions;

  @doc("The list of input messages for the completion.")
  input: ChatMessage[];

  @doc("Optional identifier for an existing conversation thread.")
  threadId?: string;

  @doc("Optional metadata associated with the completion request.")
  metadata?: Record<string>;

  @doc("Optional configuration for completion generation.")
  options?: CompletionsOptions;

  @doc("Identifier for the user making the request.")
  userId?: string;

  @doc("Flag indicating whether to store the completion and associated messages.")
  store?: boolean;
}


model AgentCompleteParams {
  @doc("The overrides for the agent responsible for generating the completion.")
  overrides: AgentOptions;

  @doc("The list of input messages for the completion.")
  input: ChatMessage[];

  @doc("Optional identifier for an existing conversation thread.")
  threadId?: string;

  @doc("Optional metadata associated with the completion request.")
  metadata?: Record<string>;

  @doc("Optional configuration for completion generation.")
  options?: CompletionsOptions;

  @doc("Identifier for the user making the request.")
  userId?: string;

  @doc("Flag indicating whether to store the completion and associated messages.")
  store?: boolean;
}

model CompletionUsage {
  @doc("Number of completion tokens used over the course of the run step.")
  outputTokens: int64;

  @doc("Number of prompt tokens used over the course of the run step.")
  inputTokens: int64;

  @doc("Total number of tokens used (prompt + completion).")
  totalTokens: int64;

  @doc("Details of the prompt tokens.")
  inputTokenDetails?: {
    @doc("The number of cached prompt tokens.")
    cachedTokens?: int32;
  };

  @doc("Breakdown of tokens used in a completion.")
  outputTokenDetails?: {
    @doc("Tokens generated by the model for reasoning.")
    reasoningTokens?: int32;
  };
}

model StreamingOperation {
  @doc("The type of operation being performed.")
  type: "append" | "remove" | "replace" | "set";

  @doc("The JSON path to the operation.")
  jsonPath: string;

  @doc("The value to apply to the operation.")
  value?: unknown;
}

model StreamingAgentCompletionUpdate {
  @doc("Identifier of the message being updated.")
  messageId: string;

  @doc("Optional name of the message author.")
  authorName?: string;

  @doc("Role of the author for the updated message.")
  authorRole?: AuthorRole;

  @doc("Optional content updates from the AI.")
  update?: StreamingOperation;

  @doc("Token usage information associated with this streaming update.")
  usage: CompletionUsage;
}

model AgentCompletion {
  @doc("Unique identifier for the agent responsible for the completion.")
  agentId: string;
  
  @doc("Unique identifier for this completion.")
  completionId: string;

  @doc("Timestamp when the completion was initiated.")
  createdAt: safeint;

  @doc("Timestamp when the completion finished processing.")
  completedAt: safeint;

  @doc("Final status of the completion request.")
  status:  "inProgress" | "incomplete" | "cancelled" | "failed" | "completed";

  @doc("List of output messages generated by the agent.")
  output: ChatMessage[];

  @doc("Identifier for the thread associated with the completion.")
  threadId: string;

  @doc("Token usage details for this completion.")
  usage: CompletionUsage;

  @doc("Details about why the response is incomplete,")
  incompleteDetails: {
    reason: string;
  }
}
