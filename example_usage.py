#!/usr/bin/env python3
"""
Example usage of the TypeSpec Assessment Tool

This example shows how to use the assessment tool to get specific insights
about TypeSpec customization patterns.
"""

import json
import subprocess
from pathlib import Path

def run_assessment_example():
    """Example showing how to use the assessment tool."""
    
    print("TypeSpec Customization Assessment Example")
    print("=" * 50)
    
    # Run the assessment tool
    print("Running assessment for last 6 months...")
    result = subprocess.run([
        "python", "typespec_assessment_tool.py", 
        "--output", "assessment_results.json"
    ], capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"Error running assessment: {result.stderr}")
        return
    
    # Load the detailed results
    with open("assessment_results.json", "r") as f:
        data = json.load(f)
    
    # Extract specific insights
    results = data["detailed_results"]
    
    print("\nSpecific Insights:")
    print("-" * 30)
    
    # Find projects with the most client customizations
    client_projects = [r for r in results if r["has_client_tsp"]]
    top_client_custom = sorted(client_projects, 
                              key=lambda x: x["client_tsp_customizations"], 
                              reverse=True)[:5]
    
    print("Top 5 projects with most client.tsp customizations:")
    for i, project in enumerate(top_client_custom, 1):
        print(f"  {i}. {project['project_path']}")
        print(f"     Customizations: {project['client_tsp_customizations']}")
    
    # Find projects with most additional files
    multi_file_projects = [r for r in results if r["additional_tsp_files"] > 0]
    top_multi_file = sorted(multi_file_projects, 
                           key=lambda x: x["additional_tsp_files"], 
                           reverse=True)[:5]
    
    print("\nTop 5 projects with most additional .tsp files:")
    for i, project in enumerate(top_multi_file, 1):
        print(f"  {i}. {project['project_path']}")
        print(f"     Additional files: {project['additional_tsp_files']}")
        print(f"     Total files: {project['total_tsp_files']}")
    
    # Find the few autogenerated projects
    autogen_projects = [r for r in results if r["classification"] == "autogenerated"]
    
    print(f"\nCompletely autogenerated projects ({len(autogen_projects)}):")
    for project in autogen_projects:
        print(f"  - {project['project_path']}")
        print(f"    Files: {project['total_tsp_files']}")
        print(f"    Score: {project['customization_score']}")
    
    # Analyze suppressions
    suppression_projects = [r for r in results if r["has_suppressions"]]
    suppression_percentage = len(suppression_projects) / len(results) * 100
    
    print(f"\nSuppression Analysis:")
    print(f"  Projects with suppressions: {len(suppression_projects)} ({suppression_percentage:.1f}%)")
    print(f"  This indicates validation issues that required manual overrides")
    
    # Clean up
    Path("assessment_results.json").unlink()
    
    print("\nConclusion:")
    print("The assessment shows that TypeSpec conversion from Swagger")
    print("requires substantial manual customization in nearly all cases.")

if __name__ == "__main__":
    run_assessment_example()